{"meta":{"title":"Welcome","subtitle":"subtitile","description":"detailed description","author":"Kira Zhang","url":"http://yoursite.com","root":"/"},"pages":[],"posts":[{"title":"Causal Bayesian Network","slug":"Causal-Bayesian-Network","date":"2020-06-20T07:04:23.000Z","updated":"2020-06-20T16:48:01.924Z","comments":true,"path":"2020/06/20/Causal-Bayesian-Network/","link":"","permalink":"http://yoursite.com/2020/06/20/Causal-Bayesian-Network/","excerpt":"","text":"[TOC] Bayesian Network贝叶斯定理 参考链接：https://www.zhihu.com/question/19725590/answer/217025594 引入贝叶斯网络的原因参考链接：https://zhuanlan.zhihu.com/p/33860572 用于简化随机变量的联合概率分布的表现 贝叶斯网络（以及其他所有的概率图模型）相比于原始的联合分布模型，最大的优势在于增加了变量之间条件独立的先验信息，从而减小了模型的体积，与模型进行推断、学习的时间。例如，上图共有5个变量，如果用朴素的联合分布模型建模，条件概率表格的体积将会是 48 ，而采用贝叶斯网络后，条件概率表格的总体积为17 。 贝叶斯网络解决的问题 条件概率 最大后验概率 贝叶斯网络的相关定义及特性参考：Causality——Judea Pearl 2nd edition Markov parents 定义： 即$PAi$是将$Xi$与其predecessor隔开的一组变量 推论：DAG是贝叶斯网络的必要条件有： 即每一个节点的概率只受其父节点的约束 Markov Compatibility If a probability function $P$ admits the factorization of (1.33) relative to DAG $G$, we say that $G$ represents $P$, that $G$ and $P$ are compatible, or that $P$ is Markov relative to $G$. 通过知道所有变量的conditional independency，就描绘与G compatible的概率分布 A convenient way of characterizing the set o f distributions compatible with a DAG G is to list the set of (conditional) independencies that each such distribution must sat­isfy. These independencies can be read off the DAG by using a graphical criterion called d-separation Parental Markov Condition 定义 Ordered Markov Condition 定义 A consequence of this theorem is an order-independent criterion for determining whether a given probability P is Markov relative to a given DAG G. order-independent：由于同一张联合分布概率表可能对应多张贝叶斯图，某一个节点的predecessor排列也有多种，无论predecessor的排列如何都，该节点都与所有的predecessor节点独立 Observational Equivalence 定义 Two networks that are observationally equivalent cannot be distinguished without resorting to manipulative experimentation or temporal information X3:Sprinkler,X2:Rain 将X1，X2中间的箭头逆转，不会破坏原有的V-structure/引入新的V-structure；因此X2/X1中箭头两个方向是observational equivalent。因此，X1和X2之间箭头的方向不能由probabilistic info得出 若对X2-X4箭头方向调整，则会破坏一个原有的V结构，X4-X5之间箭头方向调整，则会产生一个新的V结构 Thus, we see that some probability functions P (such as the one responsi­ ble for the construction of the Bayesian network in above pic), when unaccompanied by temporal information，can constrain the directionality of some arrows in the graph. Techniques for calculation Message-passing architecture and Tree structure join-tree propagation: decompose the network into clusters then form tree structure, each clusters is a compound variable that is capable of passing mes­sages to its neighbors Cut-set conditioning method(不太理解) a set of variables is instantiated (given specific values) such that the remaining network forms a tree.The propagation is then performed on that tree, and a new instantiation chosen, until all instantiations have been exhausted; the results are then averaged Independency in BN如何判断a与b的independency: 假如a发生变化，a变化的影响是否会传达到b Local Independency Any variable in the network is independent of its non-descendents given its parents.$$ (X \\perp NonDesc(X) | Pa(X) $$where $ NonDesc(X) $ is the set of variables which are not descendents of $ X $ and $ Pa(X) $ is the set of variables which are parents of $ X $. Global Independency三种结构参考链接：https://zhuanlan.zhihu.com/p/30139208 head-to-head(Common Evidence) c已知 a产生变化，c不变，那么b也要发生变化，因此a、b不独立 有： P(a,b|c)= P(a,b,c)/P(c)=P(a)P(b)P(c|a,b)/P(c) 无法得到P(a,b|c)=P(a|c)P(b|c) c未知 a、b被阻断（blocked），条件独立 tail-to-tail(Common Cause/V structure) c已知 有： P(a,b,c)=P(c)P(a|c)P(b|c) P(a,b|c)=P(a|c)P(b|c) a/b仅取决于c，a⊥b｜c c未知 无法得出P(a,b) = P(a)P(b)，即c未知时，a、b不独立。 head-to-tail c已知 a的变化不会影响c，c的变化会影响b，因此a⊥b｜c c未知 The influence flows from $ A $ to $ B $ when $ C $ is not observed 有： P(a,b,c)=P(a)P(c|a)P(b|c) 但无法推出P(a,b) = P(a)P(b) 即c未知时，a、b不独立。 xi+1的分布状态只和xi有关，和其他变量条件独立，这种顺次演变的随机过程，就叫做马尔科夫链（Markov chain） D-separation为了回答给定一个随机变量的集合Z，随机变量A与B之间是否条件独立这个问题，我们需要引入d分隔的概念 A、B间存在d分隔：某个节点集合C能d分隔节点A与节点B，当且仅当：给定C时，A与B之间不存在有效路径（active path/active trail），即A、B被C隔断（blocked），A、B条件独立。 Active Trail: 若A的变化可以影响B，则A和B中存在active trail A、B间存在d联结：不存在d分隔 例子： 《Causality》中是这样定义的：例子： 粉色标记处：collider的子节点已知会使collider所在的path unlocked，即此时X会影响Y 贝叶斯网络的创建生成贝叶斯网络初始顺序的重要性 在构建贝叶斯网络的时候，会收到变量初始状态的影响 Structure Learning 定义： The task of structure learning for Bayesian networks refers to learn the structure of the directed acyclic graph (DAG) from data 两种方式： score-based approach constraint-based approach Parameter learning 定义 Parameter learning is the process of using data to learn the distributions of a Bayesian network or Dynamic Bayesian network. Dynamic Bayesian network: 变量的概率分布会随时间发生变化 在贝叶斯参数学习里，认为模型的参数不是一个确定的值，而是一个随机变量，且满足一定的分布。贝叶斯参数学习的过程就是根据已知的数据集D来估计参数的概率分布，再用得到的参数概率分布来计算随机变量X的后验概率分布。 complete data的方法 极大似然估计MLE 概率是通过参数，推测事实；似然是根据事实，推测参数 通过不断优化参数，使得一个事件发生的概率尽可能大Maximize{P((x1,x2,x3….)|参数)} 为什么能变成连乘：因为MLE的独立同分布假设 参考链接： https://www.zhihu.com/question/24124998 https://zhuanlan.zhihu.com/p/26614750 贝叶斯估计 参考链接：https://zhuanlan.zhihu.com/p/61593112 通常我们取后验分布的期望作为参数的估计值 最大后验估计MAP： 参考链接：https://zhuanlan.zhihu.com/p/61593112 incomplete data的方法 Expectation-Maximization algorithm Robust Bayesian estimate Monte-Carlo Method Gaussian approximation method Causal Bayesian Network实例肺癌分析实例： http://www.causality.inf.ethz.ch//data/LUCAS.html 贝叶斯网络模型在Causal Inference时的局限性局限性：贝叶斯网络本身无法区分出因果的方向。例如，A←B←C与A→B→C的因果方向完全相反，但在贝叶斯网络的模型描述下，它们表达的概率分布和条件独立假设完全相同。 贝叶斯网络中，A→B未必等同于A导致B 概率论「给定/已知随机变量Z」里的「给定/已知」只能用于表达观察，而非介入（介入即产生激励/影响）。 参考“介入主义因果观”，简而言之就是因是激励，果是响应 因果关系是不可逆/非对称的，但是相关性是可逆/对称的 马尔可夫等价类 The philosophy behind caual Bayesian Networks It seems that if conditional independence judgments are byproducts of stored causal relationships, then tapping and representing those relationships directly would be a more natural and more reliable way of expressing what we know or believe about the world. Causal network的优越性 causal mod­els (assuming they are valid) are much more informative than probability models. A joint distribution tells us how probable events are and how probabilities would change with subsequent observations, but a causal model also tells us how these probabilities would change as a result of external interventions 模块性 由于 parent-child 之间的稳定以及自治的物理机制. 在不改变其他变量之间分布的情况下, 单独改变一个 parent-child 之间的分布是有可能的 ( conceivable ) Causal Bayesian network和Bayesian network区别 CBN中的操作是do/intervene，BN中的操作是observe，do可以得出直接因果关系，observe只能得出相关性 Definition of causal Bayesian networkP($v$)是集合V上的概率分布P$x$($v$)是定义在intervention do(X=$x$)上的概率分布 因果贝叶斯网络应该符合概率图模型的标准 对于干涉的部分，P(do(X=$x$))=1 被干涉变量的父节点的概率分布是不变的。（并且注意的一点:这里讲干涉的分布形式成功转换为了正常的概率分布形式） Properties of Causal Bayesian network truncated factorization：justify the deletion procedure deletion：在causal network中，do(X)相当于把X与其predecessor全部断开 保证observe的condition probability和do操作的一致 为什么要一致呢？？ 控制了$Vi$的直接原因$PAi$（一组令Vi independent of its predecessor的变量）,那么剩下的变量就不会影响$Vi$的概率分布 只有$Vi$的descendants的概率分布会受到do操作的影响 Causal relationship and their stability 为了检验$Xi$对$Xj$是否有causal influence，进行操作do(X=$Xi$) 只有$Xi$的descendant会受到intervention的影响 计算P$xi$($Xj$), 看$Xj$的分布是否发生了变化，若发生了变化则说明$Xi$对$Xj$有causal influence causal relationship的稳定性 因果关系是 ontological (存在论) 的, 即真实的因果关系不会根据环境(do-operator)的变化而变化. 即使我们队知识的认识(概率分布)发生了变化 概率关系是 epistemic (认识论) 的, 我们对事物的认识(概率分布)会随着环境的变化(do-operator)而变化 一些概念Markov Blanket MB包括：all parents, children, and spouses of the target（X） 一个节点的MB：contains all the variables that shield the node from the rest of the network. MB is the only knowledge needed to predict the behavior of that node and its children 例子： Markov Process a random process in which the future is independent of the past, given the present do操作符do(X),相当于令x已知，将x与所有指向x的点断开 Backdoor Criteria什么是存在干扰 引入Backdoor Criteria的原因 为了便于在实验的时候选择控制变量，确定对哪些变量进行intervene 在causal inference时，控制变量就是confounder（混淆变量） 参考链接：https://blog.csdn.net/s1314_JHC/article/details/80790112 Backdoor Path $A$’s backdoor path is a non-causal path from $A$ to $Y$. They are “backdoor” paths because they flow backwards out of A: all of these paths point into A. 删除所有以A为起点的path(frontdoor path)后，保留的就是backdoor path backdoor path里存在confounder 两种backdoor path结构 Common Causes A是treatment，Y是outcome，X是confounder 由于混淆表量confounder的存在，导致A-&gt;Y的因果关系受到影响 M-bias This structure is typically used to represent a circumstance where a researcher observes T,Y,M(U,W unobserved) Clearly, the causal effect of T on Y is 0 , which is also equal to the marginal association between T and Y In order to test whether M is a confounder, one would ‘adjust M’, eg: including M in a regression or by matching units on similar values of M. Then the causal expectation between T &amp; Y is not 0. So M is a confounder unblocked path / blocked path 存在unblocked backdoor path，x未知，A和Y条件不独立，A和Y有两种关系 $A$直接导致了$Y$(直接因果关系) 通过backdoor path，$A$对$Y$施加了影响 存在blocked backdoor path，有两种情况 control for or stratify a non-collider on that path, 即已知X，A与Y条件独立 backdoor path上存在collider，则这条路必blocked collider blocks the association between the variables that influence it. Condition on the collider opens the path between $X$ and $Y$,cause “Berkson’s paradox” Berkson’s paradox 现象 two values can statistically be negatively correlated even when they appear positively correlated in the population. 产生原因 caused by systematically observing some events more than others. If you know that X+Y must be within a certain range, then having a high X results in a lower Y, and vice versa. X+Y 已知相当于上述的collider已知 与confounder不一样！！confounder是在研究因果关系时的控制变量；而berkson paradox仅是一个现象 Definition of Backdoor criterion An effect of A on Y is identifiable if either: No backdoor paths from A to Y Measured covariates are sufficient to block all backdoor paths from A to Y Backdoor criterion is powerful because it can check: if there is confounding given this DAG if it is possible to removing the confounding what variables to condition on to eliminate the confounding 简而言之，backdoor criterion用于检测A到Y是否有直接的因果关系 借助backdoor criterion可以给已知DAG消歧，确定唯一的因果关系 Backdoor criterion and Ignorability Ignorability Assumption: no unmeasured confounders no unblockable backdoor paths Suppose that we use the backdoor criterion and find that a set of variables X blocks all the backdoor paths. This implies the treatment assignment is conditionally ignorable: Y (a) ⫫A|X.","categories":[],"tags":[{"name":"Causal Inference","slug":"Causal-Inference","permalink":"http://yoursite.com/tags/Causal-Inference/"}]},{"title":"Hello World","slug":"hello-world","date":"2020-06-19T14:07:24.409Z","updated":"2020-06-19T14:07:24.409Z","comments":true,"path":"2020/06/19/hello-world/","link":"","permalink":"http://yoursite.com/2020/06/19/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}],"categories":[],"tags":[{"name":"Causal Inference","slug":"Causal-Inference","permalink":"http://yoursite.com/tags/Causal-Inference/"}]}